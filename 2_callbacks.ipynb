{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants\n",
    "IMG_ROWS = 28\n",
    "IMG_COLS = 28\n",
    "NUM_CLASSES = 10\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 2018\n",
    "\n",
    "N_TRAIN_SAMPLES = 5000\n",
    "N_TEST_SAMPLES = 1000\n",
    "\n",
    "#Model\n",
    "NO_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar functions\n",
    "def data_preprocessing(x_data, y_data):\n",
    "    x_data = x_data.astype('float32')\n",
    "    out_y = to_categorical(y_data, len(np.unique(y_data)))\n",
    "    out_x = np.expand_dims(x_data, axis=-1)\n",
    "    \n",
    "    return out_x, out_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(x_train_data, y_train_data), (x_test_data, y_test_data) = fashion_mnist.load_data()\n",
    "\n",
    "# For this tutorial we will work on a smaller subset\n",
    "train_idxs = np.random.choice(np.arange(x_train_data.shape[0]), size=N_TRAIN_SAMPLES, replace=False)\n",
    "test_idxs = np.random.choice(np.arange(x_test_data.shape[0]), size=N_TEST_SAMPLES, replace=False)\n",
    "\n",
    "x_train_data = x_train_data[train_idxs, :, :]\n",
    "y_train_data = y_train_data[train_idxs]\n",
    "x_test_data = x_test_data[test_idxs, :, :]\n",
    "y_test_data = y_test_data[test_idxs]\n",
    "\n",
    "# prepare the data\n",
    "X, y = data_preprocessing(x_train_data, y_train_data)\n",
    "X_test, y_test = data_preprocessing(x_test_data, y_test_data)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "# Add convolution 2D\n",
    "model.add(Conv2D(16, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 input_shape=(IMG_ROWS, IMG_COLS, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, \n",
    "                 kernel_size=(3, 3), \n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data generator\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1./255,  # We also can make a rescale on the data\n",
    "    horizontal_flip=True,\n",
    "    rotation_range = 60,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2)\n",
    "\n",
    "# Validation data generator\n",
    "datagen_val = ImageDataGenerator(\n",
    "    rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Callbacks\n",
    "\n",
    "A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. You can pass a list of callbacks (as the keyword argument callbacks) to the .fit() method of the Sequential or Model classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelCheckpoint\n",
    "\n",
    "Save the model after every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath='weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                    monitor='val_loss', \n",
    "                    verbose=1, \n",
    "                    save_best_only=False, \n",
    "                    save_weights_only=False, \n",
    "                    mode='auto')\n",
    "]\n",
    "\n",
    "# Train!\n",
    "train_model = model.fit_generator(\n",
    "    datagen_train.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    epochs=NO_EPOCHS,\n",
    "    validation_data=datagen_val.flow(X_val, y_val, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "    validation_steps=X_val.shape[0] // BATCH_SIZE,\n",
    "    callbacks=callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EarlyStopping\n",
    "\n",
    "Stop training when a monitored quantity has stopped improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', \n",
    "                  min_delta=0, \n",
    "                  patience=2, \n",
    "                  verbose=1, \n",
    "                  mode='auto', \n",
    "                  baseline=None, \n",
    "                  restore_best_weights=False)\n",
    "]\n",
    "\n",
    "# Train!\n",
    "train_model = model.fit_generator(\n",
    "    datagen_train.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    epochs=NO_EPOCHS,\n",
    "    validation_data=datagen_val.flow(X_val, y_val, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "    validation_steps=X_val.shape[0] // BATCH_SIZE,\n",
    "    callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "\n",
    "TensorBoard basic visualizations.\n",
    "\n",
    "[TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) is a visualization tool provided with TensorFlow.\n",
    "\n",
    "This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model.\n",
    "\n",
    "If you have installed TensorFlow with pip, you should be able to launch TensorBoard from the command line:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=/full_path_to_your_logs\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "callbacks = [\n",
    "    TensorBoard(log_dir='./logs', \n",
    "                write_graph=True, \n",
    "                write_images=False)\n",
    "]\n",
    "\n",
    "# Train!\n",
    "train_model = model.fit_generator(\n",
    "    datagen_train.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    epochs=NO_EPOCHS,\n",
    "    validation_data=datagen_val.flow(X_val, y_val, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "    validation_steps=X_val.shape[0] // BATCH_SIZE,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSVLogger\n",
    "\n",
    "Callback that streams epoch results to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "callbacks = [\n",
    "    CSVLogger(filename='train_logs.csv', \n",
    "              separator=',', \n",
    "              append=False)\n",
    "]\n",
    "\n",
    "# Train!\n",
    "train_model = model.fit_generator(\n",
    "    datagen_train.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    epochs=NO_EPOCHS,\n",
    "    validation_data=datagen_val.flow(X_val, y_val, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "    validation_steps=X_val.shape[0] // BATCH_SIZE,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReduceLROnPlateau\n",
    "\n",
    "Reduce learning rate when a metric has stopped improving.\n",
    "\n",
    "Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', \n",
    "                      factor=0.1, \n",
    "                      patience=10, \n",
    "                      verbose=0, \n",
    "                      mode='auto')\n",
    "]\n",
    "\n",
    "train_model = model.fit_generator(\n",
    "    datagen_train.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    epochs=NO_EPOCHS,\n",
    "    validation_data=datagen_val.flow(X_val, y_val, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "    validation_steps=X_val.shape[0] // BATCH_SIZE,\n",
    "    callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
