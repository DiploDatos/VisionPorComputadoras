{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><font size=\"6\">Transfer Learning with Keras for Cifar-10</font></center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup one GPU for tensorflow\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# The GPU id to use, \"0\", \"1\", etc.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "# https://keras.io/applications/#documentation-for-individual-models\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Limit tensorflow gpu usage\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "sess = tf.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "NUM_CLASSES = 10\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 2018\n",
    "\n",
    "N_TRAIN_SAMPLES = 5000\n",
    "N_TEST_SAMPLES = 1000\n",
    "\n",
    "#Model\n",
    "NO_EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "NET_IMG_ROWS = 224\n",
    "NET_IMG_COLS = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_data, y_train_data), (x_test_data, y_test_data) = cifar10.load_data()\n",
    "\n",
    "# Create a dictionary for each type of label \n",
    "LABELS = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this tutorial we will work on a smaller subset\n",
    "train_idxs = np.random.choice(np.arange(x_train_data.shape[0]), size=N_TRAIN_SAMPLES, replace=False)\n",
    "test_idxs = np.random.choice(np.arange(x_test_data.shape[0]), size=N_TEST_SAMPLES, replace=False)\n",
    "\n",
    "x_train_data = x_train_data[train_idxs, :, :, :]\n",
    "y_train_data = y_train_data[train_idxs]\n",
    "x_test_data = x_test_data[test_idxs, :, :, :]\n",
    "y_test_data = y_test_data[test_idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cifar-10 x_train shape: {}\".format(x_train_data.shape))\n",
    "print(\"Cifar-10 y_train shape: {}\".format(y_train_data.shape))\n",
    "print(\"Cifar-10 x_test shape: {}\".format(x_test_data.shape))\n",
    "print(\"Cifar-10 y_test shape: {}\".format(y_test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(x_data, y_data):\n",
    "    out_x = np.array([cv2.resize(img, (NET_IMG_COLS, NET_IMG_ROWS)).astype(np.float32)\n",
    "                      for img in x_data])\n",
    "    out_y = to_categorical(y_data, len(np.unique(y_data)))\n",
    "    \n",
    "    return out_x, out_y\n",
    "\n",
    "# prepare the data\n",
    "X, y = data_preprocessing(x_train_data, y_train_data)\n",
    "X_test, y_test = data_preprocessing(x_test_data, y_test_data)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model with weights pre-trained on ImageNet.\n",
    "# We will use the Resnet50 model\n",
    "# - [Deep Residual Learning for Image Recognition](\n",
    "#    https://arxiv.org/abs/1512.03385)\n",
    "base_model = ResNet50(input_shape=(NET_IMG_ROWS, NET_IMG_COLS, 3),  # Input image size\n",
    "                      weights='imagenet',                           # Use imagenet pre-trained weights\n",
    "                      include_top=False,                            # Drop classification layer\n",
    "                      pooling='avg')                                # Global AVG pooling for the \n",
    "                                                                    #  output feature vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the model\n",
    "\n",
    "Let's check the model we initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the output feature vector from the base model\n",
    "x = base_model.output\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the final model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data generator\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    )\n",
    "\n",
    "# Validation data generator\n",
    "datagen_val = ImageDataGenerator(\n",
    "    rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional layers\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train!\n",
    "train_model = model.fit_generator(\n",
    "    datagen_train.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    epochs=NO_EPOCHS,\n",
    "    validation_data=datagen_val.flow(X_val, y_val, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "    validation_steps=X_val.shape[0] // BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will finetune from add_9\n",
    "for layer in model.layers[:99]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[99:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train!\n",
    "train_model = model.fit_generator(\n",
    "    datagen_train.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    epochs=NO_EPOCHS,\n",
    "    validation_data=datagen_val.flow(X_val, y_val, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "    validation_steps=X_val.shape[0] // BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
